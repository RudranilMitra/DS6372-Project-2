---
title: "DS6372_Project2"
author: "Samuel Onalaja, Rudranil, Neil Benson"
date: "11/11/2020"
output:
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Library

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(corrplot)
library(mlbench)
library(caret)
library(skimr)
library(mice)
library(purrr)
library(ggplot2)
library(ggthemes)
library(cowplot)
library(class)
library(e1071)
library(grid)
library(tidyr)
library(stringr)
library(naniar)
library(car)
library(MASS)
library(caret)
library(ROCR)
library(glmnet)
library(bestglm)
library(car)
library(ResourceSelection)
library(randomForest)
library('SmartEDA')

```
## Introduction
<!-- What factors impact life expectancy?  Can life expectancy be predicted?  The World Health Organization (WHO) maintains a database with life expectancy information for all countries by year along with other variables that could contribute to life expectancy.  The data falls into four main areas:  health, economic, social, and immunization.  This project makes use of the WHO data to better understand life expectancy and how to predict it.  In this paper, three different models will be explored: -->
<!-- 1)      A regression model to identify key relationships between life expectancy and factors related to health, economic, social, and immunization. -->
<!-- 2)      A parametric regression model to predict life expectancy. -->
<!-- 3)      A non-parametric regression model to predict life expectancy. -->


## Data Description
<!-- The Life Expectancy dataset used in this analysis was collected from the WHO and made available at Kaggle:  https://www.kaggle.com/kumarajarshi/life-expectancy-who.  It includes data from 193 countries for the years 2000 â€“ 2015.  The 2938 rows contained in the dataset include average life expectancy per country for each year.  In addition to the average life expectancy, there are 21 columns of data related to health, economic, social, and immunization.    -->
  
  
##Read in datafile
```{r}

library(readr)

bank_additional_full <- read_delim("https://raw.githubusercontent.com/RudranilMitra/DS6372-Project-2/master/data/bank-additional-full.csv", ";", escape_double = FALSE, trim_ws = TRUE)


head(bank_additional_full)

# creating my own not in operator
`%notin%` <- Negate(`%in%`)

# removing duration
bank_additional_full <- bank_additional_full[,-11]
```
  

## Data Summary statistics
Notice all categorical variables are in character type so we have to convert all to factor.
```{r}
summary(bank_additional_full)
dim(bank_additional_full)
names(bank_additional_full)
str(bank_additional_full)

```
  

## Convert all character type variable to factor
```{r}

for (i in seq_along(bank_additional_full)){
    if(is.character(bank_additional_full[[i]])){
        bank_additional_full[[i]]=as.factor(bank_additional_full[[i]])   
    }
}

str(bank_additional_full)
```


## Addressing missing values
The data set is completely observed, No missing values
```{r}

vis_miss(bank_additional_full) #This function is from package naniar and very effective for visualizing missing values
md.pattern(bank_additional_full) #This function also address the pattern of a missing value, its from the The MICE package (stands for Multiple Imputation by Chained Equations)

n_cols = dim(bank_additional_full)[2]
for (i in 1:n_cols)
{
  print(c(colnames(bank_additional_full[i]), sum(is.na(bank_additional_full[i]))))
}

```



##EDA
Notice some errors while trying to get a correlation so it was discovered that the record "No and Yes" in the "y" variable needs to be converted to "0" and "1"
Notice how the response variable (y) is skewed towards "0" which is no at over 89%. We will review and address this imbalance later.
```{r warning=FALSE,message=FALSE}
bank <- bank_additional_full %>%
  mutate(y = ifelse(y=="yes", 1, 0))

bank$y <- as.factor(bank$y)

table(bank$y)
prop.table(table(bank$y))

dim(bank)
str(bank)

#Observe data distribution of the response variable

table(bank$y)
prop.table(table(bank$y))


####### Check of the range of some of these continuous variables with histograms

###Campaign distribution
#Campaign is rightly skewed and contains some few outliers

ggplot(data=bank,aes(x=campaign)) + geom_histogram() +
  stat_function(fun = dnorm, args = list(mean = mean(bank$campaign, na.rm = TRUE), sd = sd(bank$campaign, na.rm = TRUE))) +
  labs(x = "campaign", y = "Count", title = "Campaign distribution") +   theme_economist()

#checking out the distribution by the response variable
#Distribution is still rightly skewed.
ggplot(bank) + geom_histogram(aes(x = campaign), binwidth = 0.1, col = "white") + facet_grid(y~., scales = "free") + scale_x_log10() +theme_bw()

summary(bank$campaign)


# bank$lgcampaign = log(log(bank$campaign))


#####Pdays distribution
# It doesn't look like pday has much information as it only shows 2 values for "0' and "1000"

ggplot(data=bank,aes(x=pdays)) + geom_histogram() +
  stat_function(fun = dnorm, args = list(mean = mean(bank$campaign, na.rm = TRUE), sd = sd(bank$pdays, na.rm = TRUE))) +
  labs(x = "pdays", y = "Count", title = "Pdays distribution") +   theme_economist()

#checking out the pdays by the response variable
#pdays is still rightly skewed.
#
ggplot(bank) + geom_histogram(aes(x = pdays), binwidth = 0.1, col = "white") + facet_grid(y~., scales = "free") + scale_x_log10() +theme_bw()

summary(bank$pdays)



####Previous distribution
ggplot(data=bank,aes(x=previous)) + geom_histogram() +
  stat_function(fun = dnorm, args = list(mean = mean(bank$previous, na.rm = TRUE), sd = sd(bank$previous, na.rm = TRUE))) +
  labs(x = "previous", y = "Count", title = "Previous distribution") +   theme_economist()

#checking out the Previous distribution by the response variable
ggplot(bank) + geom_histogram(aes(x = previous), binwidth = 0.1, col = "white") + facet_grid(y~., scales = "free") + scale_x_log10() +theme_bw()

summary(bank$pdays)


####Emp.var.rate distribution
ggplot(data=bank,aes(x=emp.var.rate)) + geom_histogram() +
  stat_function(fun = dnorm, args = list(mean = mean(bank$emp.var.rate, na.rm = TRUE), sd = sd(bank$emp.var.rate, na.rm = TRUE))) +
  labs(x = "emp.var.rate", y = "Count", title = "Emp.var.rate distribution") +   theme_economist()


#Cons.price.idx distribution
ggplot(data=bank,aes(x=bank$cons.price.idx)) + geom_histogram() +
  stat_function(fun = dnorm, args = list(mean = mean(bank$emp.var.rate, na.rm = TRUE), sd = sd(bank$cons.price.idx, na.rm = TRUE))) +
  labs(x = "cons.price.idx", y = "Count", title = "Cons.price.idx distribution") +   theme_economist()


#Cons.conf.idx distribution
ggplot(data=bank,aes(x=bank$cons.conf.idx)) + geom_histogram() +
  stat_function(fun = dnorm, args = list(mean = mean(bank$cons.conf.idx, na.rm = TRUE), sd = sd(bank$cons.conf.idx, na.rm = TRUE))) +
  labs(x = "cons.conf.idx", y = "Count", title = "Cons.conf.idx distribution") +   theme_economist()


#Euribor3m distribution
ggplot(data=bank,aes(x=bank$euribor3m)) + geom_histogram() +
  stat_function(fun = dnorm, args = list(mean = mean(bank$euribor3m, na.rm = TRUE), sd = sd(bank$euribor3m, na.rm = TRUE))) +
  labs(x = "euribor3m", y = "Count", title = "Euribor3m distribution") +   theme_economist()


#Nr.employed distribution
ggplot(data=bank,aes(x=bank$nr.employed)) + geom_histogram() +
  stat_function(fun = dnorm, args = list(mean = mean(bank$nr.employed, na.rm = TRUE), sd = sd(bank$nr.employed, na.rm = TRUE))) +
  labs(x = "nr.employed", y = "Count", title = "Nr.employed distribution") +   theme_economist()


#checking for correlation Numerical variable vs response
Attr <- "y"

# Name explanatory variable
ExplVar<- bank%>% keep(is.numeric) %>% colnames

# Create function
PlotFunc <- function(df, explanatory, response) {
  ggplot(data = df) + geom_density(aes_string(x = explanatory), alpha = 0.5) + xlab(explanatory) + ylab("subscribed")
}
  # Density plot
PlotFunc(bank, explanatory =  "age", response = "y")

#  Create plot list for plot_grid function to reference
PlotList <- lapply(ExplVar, function(x) PlotFunc(bank, x, y))

#  Grid of all categorical variables plotted against Attrition
plot_grid(plotlist = PlotList)



```


## Factor vs response variable
```{r}

summary(bank$month)
# month only has 10 levels, missing jan and feb
bank %>% ggplot(aes(month,fill=y)) + geom_bar(position="dodge") 
bank %>% ggplot(aes(month,fill=y)) + geom_bar(position="fill") + ylab("Proportion")


summary(bank$education)
# 4% "unknown" values
bank %>% ggplot(aes(education,fill=y)) + geom_bar(position="dodge")
bank %>% ggplot(aes(education,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
length(grep("illiterate",bank$education))

summary(bank$day_of_week)
#Day of the week has 5 lrvrls, saturday and sunday were excluded.
bank %>% ggplot(aes(day_of_week,fill=y)) + geom_bar(position="dodge") 
bank %>% ggplot(aes(day_of_week,fill=y)) + geom_bar(position="fill") + ylab("Proportion")


summary(bank$job)
# <330 "unknown" values
bank %>% ggplot(aes(job,fill=y)) + geom_bar(position="dodge")
bank %>% ggplot(aes(job,fill=y)) + geom_bar(position="fill") + ylab("Proportion")



#Pdays doesn't really look like a continuous variable as it only have a lot of 999's and a few value around 30 and below
summary(bank$pdays)
bank %>% ggplot(aes(pdays,fill=y)) + geom_histogram(position="dodge",binwidth=500)
bank %>% filter(pdays<999) %>% ggplot(aes(pdays,stat(density),fill=y)) + geom_histogram(position="dodge")


# because this variable is so unbalanced and more than 95% of them are value 999, there isn't much value to including this variable - it's not really telling us much, if anything, and 999 seems like it should be null, or unknown
ftable(addmargins(table(bank$y,bank$pdays)))


summary(bank$campaign)
bank %>% ggplot(aes(campaign,fill=y)) + geom_histogram(position="fill",binwidth=11)
bank %>% ggplot(aes(campaign,fill=y)) + geom_histogram(position="fill",binwidth=0.5)



```


This table shows the correlation between the numerical variables

 - nr.employed and emp.var.rate are 91% correlated. 
 - nr.employed and euribor3m are 95% correlated.
 - emp.var.rate and euribor3m are 97% correlated.
 - cons.price.idx and emp.var.rate are 78% correlated.
 - cons.price.idx and euribor3m are 69% correlated.
 - cons.price.idx and nr.employed are 52% correlated.
 
Later we will examine pairwise multicolinearity within the continuous explanatory variables and VIF to see if which explanatory variables may be redundant.

```{r}
corrdfTraintable <- bank %>% keep(is.numeric) %>% na.omit %>% cor %>% view

bank %>% keep(is.numeric) %>% na.omit %>% cor %>% corrplot("upper", addCoef.col = "black", number.digits = 2, number.cex = 0.5, method="shade", order = "hclust", tl.srt=45, tl.cex = 0.8)

view(corrdfTraintable)

```


## Using the package SmartEda for data exploration
- check out the distribution of numerical variable and categorical variable.  
```{r}


ExpData(data = bank, type = 1)

BankPlot <- ExpNumViz(bank, target = NULL, nlim = 10, Page = c(3,3), sample = 8)

BankPlotn <- ExpNumViz(bank, target = "y", nlim = 10, Page = c(3,3), sample = 8)

Bankplot2 <- ExpCatViz(bank, target= "y", clim=10, margin=2, Page = c(3,3), sample=8)

pairs(bank %>% keep(is.numeric) %>% na.omit, col = bank$y)
```
We don't observe any multicolinearity within the numeric responses, and don't see any need to remove any based on pairwise comparison. 

```{r}

my.cor <- cor(bank %>% keep(is.numeric) %>% na.omit)

library(gplots)
library(ggplot2)
heatmap.2(my.cor,col=redgreen(75), 
          density.info="none", trace="none", dendrogram=c("row"), 
          symm=F,symkey=T,symbreaks=T, scale="none")


#Another option here would be to do PCA among the continous predictors to see
#if they seperate out.  Or a heatmap.
pc.result<-prcomp(bank %>% keep(is.numeric) %>% na.omit,scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$y<-bank$y


#Use ggplot2 to plot the first few pc's
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of y")

ggplot(data = pc.scores, aes(x = PC2, y = PC3)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of y")

ggplot(data = pc.scores, aes(x = PC3, y = PC4)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of y")

# we don't really see much separation with PCA

```

Missing values are disguised as unknown values in the data set and we devised a code to show them all

 default           8597
 education         1731
 housing            990
 loan               990
 job                330
 marital             80
 
 For Marital, housing and job it is safe to remove the unknown values as they're so little they won't have an effect on the rest of the distribution
 
 - Looking at default is has no information as it is highly skewed towards "no" as only 3 counts comes up as "yes" so we are removing "default" as well.
 
 
 
 
```{r}
bank %>% 
  summarise_all(list(~sum(. == "unknown"))) %>% 
  gather(key = "variable", value = "nr_unknown") %>% 
  arrange(-nr_unknown)

summary(bank$job)
summary(bank$default)
summary(bank$education)
summary(bank$loan)
summary(bank$marital)
summary(bank$housing)

bank <- subset(bank, job!="unknown")
bank <- subset(bank, marital!="unknown")
bank <- subset(bank, housing!="unknown")
bank <- subset(bank, loan!="unknown")
bank <- subset(bank, default!="unknown")
bank <- subset(bank, education!="unknown")


```



```{r}

library("dplyr")
ShowPieChart <- function(columnBy, columnToShow, titleName)
{
  df <- dplyr::group_by(bank, .dots = c(columnBy, columnToShow)) %>%
    dplyr::summarize(counts = n()) %>%
    dplyr::mutate(perc = (counts / sum(counts)) * 100) %>%
    dplyr::arrange_(.dots=c(columnBy, columnToShow))
 
  
  # preparing the plot
  ggplot2::ggplot(df, aes('', counts)) +
    geom_col(
      position = 'fill',
      color = 'black',
      width = 1,
      aes(fill = y)
    ) +
    ggtitle(titleName) +
    facet_wrap(paste("~",columnBy), labeller = "label_both") +
    geom_label(
      aes(label = paste0(round(perc), "%"), group = "y"),
      position = position_fill(vjust = 0.5),
      color = 'black',
      size = 5,
      show.legend = FALSE
    ) + scale_fill_discrete(name = "Outcome:") +
    coord_polar(theta = "y")
}
ShowPieChart("job", "y", "Outcome by Job")
ShowPieChart("marital", "y", "Outcome by Marital Status")
ShowPieChart("education", "y", "Outcome by Education")
ShowPieChart("housing", "y", "Outcome by Housing")
ShowPieChart("default", "y", "Outcome by Credit In Default")
ShowPieChart("loan", "y", "Outcome by loan status")
ShowPieChart("contact", "y", "Outcome by Contact")
ShowPieChart("poutcome", "y", "Outcome by poutcome")
 
```


GLM Logistic
 Below are the variables that have VIF greater than 10
-emp.var.rate
-nr.employed
-euribor3m
-cons.price.idx

Because `emp.var.rate`, `nr.employed`, `euribor3m`, and `cons.price.idx` are so highly correlated with each other, and all have high VIFs, we will start by removing `nr.employed` first, and re-evaluate. 

```{r}
cols <- c(colnames(bank))

# removing columns that do not provide any value and the response
cols <- cols[cols %notin% c("y","default","lgcampaign","pdays")]

fmla.all <- as.formula(paste("y ~ ", paste(cols, collapse= "+")))

bankModel <- glm(fmla.all, bank, family = binomial(link="logit"))

(vif(bankModel)[,3])^2


# removing columns that are highly correlated and with high VIF
cols.lowvif <- cols[cols %notin% c("nr.employed","emp.var.rate")]

# creating the formula for a full model after removing the necessary variables
fmla.lowvif <- as.formula(paste("y ~ ", paste(cols.lowvif, collapse= "+")))

bankModel.lowvif <- glm(fmla.lowvif, bank, family = binomial(link="logit"))

# re-examining the VIF after removing variables with high VIF
(vif(bankModel.lowvif)[,3])^2

```


## Create Training and Test Samples
From the bank data, we will break the data set up into training and test to fit our basic models
```{r}
sample <- sample(c(TRUE, FALSE), nrow(bank), replace=TRUE, prob=c(0.7,0.3))
train_bank <- bank[sample, ]
test_bank <- bank[!sample, ]  

```


## Checking the Balance of the Data
Because of the imbalance in the data, we will later down sample to balance the data to train our models.
```{r,echo=FALSE,warning=FALSE,message=FALSE}
print(bank %>% count(bank$y))
```


## Down sampling the data
Down sampling the training data set
```{r}
# split the dataframe into those who attritioned and those who did not to create a general overall profile of the two
y0 <- train_bank %>% filter(y == "0")
y1 <- train_bank %>% filter(y == "1")


# downsampling to balance y; Will also use this seed for creating training and test sets
set.seed(43)
      

# sampling the data for y=0
sampleIndices <- sample(seq(1:nrow(y0)),nrow(y1))
y0sampleDF<- y0[sampleIndices,]

downsample_bank_DF <- rbind(y0sampleDF, y1)

```



## Models
```{r,warning=FALSE,message=FALSE}

# building the formula for a full model
fmla.full <- fmla.lowvif

# define intercept-only model
intercept_only_model <- glm(y ~ 1, data = train_bank, family="binomial")

# define total model
total_model <- glm(fmla.full, data = train_bank, family="binomial")


# set the different models using a variety of feature selection methods.
bank_back_model <- step(total_model, 
                          direction = "backward", trace=FALSE)

bank_step_model <- step(intercept_only_model, 
                          direction = "both", scope = formula(total_model), trace=FALSE)

bank_fwd_model <- step(intercept_only_model, 
                         direction = "forward", scope = formula(total_model), trace=FALSE)
```


# Function to test models against a test set
```{r}
# function to test the models. Returns accuracy, sensitivity, and specificity of the test data set
modelOptimization <- function(train_model, test_dataframe, ResponseCol, threshold){
      # this function takes the trained model and test data set
      # and runs the training model on the test. It returns the accuracy, sensitivity,
      # and specificity for each model as a named list
      # parameters: 
          # train_model: the predictive model fit to a training set
          # test_dataframe: the test set of data as a dataframe
          # ResponseCol: the name of the response column represented as a string i.e. "y"
          # threshold: The threshold for which a prediction is considered correct i.e. above .5 or above .1
  
  
      cols <- as.vector(strsplit(Reduce(paste, deparse(train_model[["terms"]][[3]])), " +")[[1]])
      cols <- c(cols[cols %notin% c("+")])

      # testing the model's prediction
      test_dataframe$pred_response <- predict(train_model,test_dataframe[, cols, drop=FALSE],type="response")
      test_dataframe$pred_response = ifelse(test_dataframe$pred_response > threshold,"1","0")
      
      # create table for the confusion matrix
      cmtable <- table(test_dataframe$pred_response,test_dataframe[[ResponseCol]])
      
      # if there are missing rows (if the model has only predicted all yes or all no)
      # then append yes or no row of 0's
      if(nrow(cmtable) < 2) {
        if ("Yes" %in% rownames(cmtable))
          {
            cmtable <- as.table(rbind(cmtable, "0"=as.integer(c(0, 0))))

          }
        else
          {
            cmtable <- as.table(rbind(cmtable, "1"=as.integer(c(0, 0))))
          }
      }
      
      CM = confusionMatrix(cmtable)
      
      misclassification <- (cmtable[1,2]+cmtable[2,1])/(cmtable[1,1]+cmtable[2,2]+cmtable[1,2]+cmtable[2,1])
      
      type1error_rate <- (cmtable[1,2])/(cmtable[1,1]+cmtable[2,2]+cmtable[1,2]+cmtable[2,1])
      type2error_rate <- (cmtable[2,1])/(cmtable[1,1]+cmtable[2,2]+cmtable[1,2]+cmtable[2,1])
      
      returnlist <- c(CM$overall["Accuracy"], CM$byClass["Sensitivity"], CM$byClass["Specificity"], Misclassification=misclassification, Type1error_rate=type1error_rate, Type2error_rate=type2error_rate)
      
      df <- data.frame(matrix(unlist(returnlist), nrow=1, byrow=T),stringsAsFactors=FALSE)
      
      return(returnlist)
      
}
```


## Applying the trained models to the test sets
And reviewing their predictive performance against the test set with a threshold of .5
```{r}
threshold <- .5

# adding the output from modelOptimization function to a dataframe to compare statistics about each model.
bank_back_model_fmla <- bank_back_model[["formula"]]
back_model_row <- c(Model="Back Model", modelOptimization(bank_back_model,test_bank,"y", threshold), fmla=bank_back_model_fmla)


bank_step_model_fmla <- bank_step_model[["formula"]]
step_model_row <- c(Model="Step Model", modelOptimization(bank_step_model,test_bank,"y", threshold), fmla=bank_step_model_fmla)


bank_fwd_model_fmla <- bank_fwd_model[["formula"]]
fwd_model_row <- c(Model="Fwd Model", modelOptimization(bank_fwd_model,test_bank,"y", threshold), fmla=bank_fwd_model_fmla)

model_comparison_df <- rbind(back_model_row, step_model_row, fwd_model_row)
model_comparison_df <- data.frame(model_comparison_df)

view(model_comparison_df)
```
All 3 feature selection methods reduced the variables down to the same set of variables for each, and all three produced the same accuracy, misclassifaction, sensitivity, specificity, false positive rate, and false negative rate when applied to a test data set. They are the same by all definitions.

The model we will move forward with for interpretation is as follows:
`y ~ euribor3m + month + poutcome + job + campaign + marital + previous + cons.conf.idx + cons.price.idx`  
  
  
#### Summary of the main interpretable model
against the training data set  
```{r,echo=FALSE,warning=FALSE,message=FALSE}
fmla.main <- as.formula(bank_fwd_model[["formula"]])

model.main <- glm(fmla.main, data = train_bank, family = binomial(link="logit"))

summary(model.main)
```


```{r}
(vif(model.main)[,3])^2
hoslem.test(model.main$y, fitted(model.main), g=10)
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
vif(model.main)
```


## #Residual Diagnostics
```{r}
plot(model.main)
```


## ROC Curve â€“ Against Test Data Set
```{r}

```